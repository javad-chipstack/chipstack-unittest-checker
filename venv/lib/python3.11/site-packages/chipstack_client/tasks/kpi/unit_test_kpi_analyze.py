import csv
import json
import os

from chipstack_client.tasks.unit_tests import UnitTests


class UnitTestAnalyze:
    """This class takes in a UnitTests object and analyzes the performance to provide KPIS for the test run.

    A key assumption is that the provided UnitTests object has already generated, fixed, and run the unit tests.
    """

    def __init__(self, unit_tests_inst: UnitTests):
        self.unit_tests_inst = unit_tests_inst
        self.kpi = [
            [
                "Design Name",
                "Scenarios Generated",
                "Scenarios w/ Syntax Errors After First Generation",
                "Scenarios w/ Syntax Errors Remaining",
                "Run Status",
                "Passed Scenarios",
                "Failed Scenarios",
                "Total Coverage",
            ],
        ]

    def analyze(self):
        """
        Analyze the performance of the unit tests.

        Returns:
            None

        """
        design_name = self.unit_tests_inst.parsed_design["name"]
        num_scenarios = len(self.unit_tests_inst.scenarios)
        # Number of scenarios that had a syntax error after the first generation
        num_scenarios_with_syntax_error = 0
        for codeblock in self.unit_tests_inst.syntax_correction.codeblocks:
            if codeblock.correction_history[0].get_num_errors() > 0:
                num_scenarios_with_syntax_error += 1
        # Number of scenarios thatcould not be fixed
        num_scenarios_not_fixed = len(
            [
                cb
                for cb in self.unit_tests_inst.syntax_correction.codeblocks
                if not cb.is_syntax_clean
            ]
        )
        # Check results
        if self.unit_tests_inst.test_flow == "Simulation":
            test_status = self.unit_tests_inst.run_results.get("status", "Fail")
            tests_results = self.unit_tests_inst.run_results.get("log", {}).get(
                "tests", []
            )
            num_scenarios_passed = len(
                [test for test in tests_results if test["result"] == "PASSED"]
            )
            total_scenarios = len(tests_results)
            total_coverage = self.unit_tests_inst.run_results.get("coverage", {}).get(
                "Block Average", "Not Reported"
            )
        else:
            errors = self.unit_tests_inst.run_results.get("log", {}).get("errors", [])
            test_status = "Fail" if errors else "Pass"
            if not self.unit_tests_inst.run_results:
                num_scenarios_passed = 0
                total_scenarios = 0
                total_coverage = 0
            else:
                run_results = self.unit_tests_inst.run_results.get("result", {})
                run_results = run_results or {}
                assertion_results = run_results.get("results", {}).get("assert", {})
                num_scenarios_passed = len(
                    [
                        assertion
                        for assertion in assertion_results
                        if assertion.get("Result", "cex") == "proven"
                    ]
                )
                total_scenarios = len(assertion_results)
                total_coverage = run_results.get("coverage", {}).get("summary", 0)

        self.kpi.append(
            [
                design_name,
                num_scenarios,
                num_scenarios_with_syntax_error,
                num_scenarios_not_fixed,
                test_status,
                num_scenarios_passed,
                total_scenarios - num_scenarios_passed,
                total_coverage,
            ]
        )

    def dump_unit_test_metadata(self, output_dir: str) -> list:
        """
        Dump the results to the output directory.

        Returns:
            None

        """
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # Save all the key results to the output directory
        with open(os.path.join(output_dir, "parsed_design.json"), "w") as f:
            json.dump(self.unit_tests_inst.parsed_design, f, indent=4)
        with open(os.path.join(output_dir, "mental_model.json"), "w") as f:
            json.dump(self.unit_tests_inst.mental_model, f, indent=4)
        with open(os.path.join(output_dir, "scenarios.json"), "w") as f:
            json.dump(self.unit_tests_inst.scenarios, f, indent=4)
        with open(os.path.join(output_dir, "testbench_before_syntax_fix.sv"), "w") as f:
            f.write(self.unit_tests_inst.testbench)
        with open(os.path.join(output_dir, "testbench_after_syntax_fix.sv"), "w") as f:
            f.write(self.unit_tests_inst.testbench_after_syntax_fix)
        with open(os.path.join(output_dir, "syntax_correction.json"), "w") as f:
            json.dump(
                self.unit_tests_inst.syntax_correction.dump_metadata(), f, indent=4
            )
        with open(os.path.join(output_dir, "run_results.json"), "w") as f:
            json.dump(self.unit_tests_inst.run_results, f, indent=4)

        self.store_kpi_results(os.path.join(output_dir, "kpi_results.csv"))

        return self.kpi[-1]

    def store_kpi_results(self, output_path: str):
        """
        Store the KPI results as CSV.

        Returns:
            None

        """
        # write out CSV
        with open(output_path, "w") as f:
            writer = csv.writer(f)
            writer.writerows(self.kpi)
