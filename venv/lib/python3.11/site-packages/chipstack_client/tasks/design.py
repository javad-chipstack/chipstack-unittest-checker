import os
import json
import re
import tempfile
import shutil
from typing import List, Tuple, Dict, Any, TextIO
from pydantic import BaseModel, ConfigDict

from chipstack_client.client_v2 import ClientV2
from chipstack_client.models.syntax_correction import SyntaxIssues, CorrectionContext
from chipstack_client.models.logging import (
    FormalSyntaxCorrectionLogs,
    FormalCorrectionContextLog,
    SyntaxResultTypeEnum,
    FormalCorrectionLog,
)
from chipstack_client.utils.reporting import text_to_html


def generate_unit_tests(
    top_rtl_path: str,
    dependency_paths: list[str],
    unit_test_flow: str = "Formal",
):
    """
    Generates unit tests for a given RTL file path for all .sv files in the directory.

    Args:
        top_rtl_path (str): The path to the RTL file.
        dependency_paths (list[str]): The paths to the dependency RTL files.
        unit_test_flow (str, optional): The type of test to generate. Defaults to "Formal".
    """

    client_v2 = ClientV2()
    top_file = open(top_rtl_path, "r")
    dependency_files: List[TextIO] = [open(file, "r") for file in dependency_paths]
    try:
        top_file.seek(0)
        parsed_design = client_v2.parse_design(
            top_file=top_file, dependency_files=dependency_files
        )
    except Exception as error:
        raise RuntimeError(f"Error parsing design: {error}") from error

    try:
        top_file.seek(0)
        mental_model = client_v2.generate_mental_model(
            top_file=top_file,
            parsed_design=parsed_design,
            unit_test_flow=unit_test_flow,
        )
    except Exception as error:
        raise RuntimeError(f"Error generating mental model: {error}") from error

    try:
        top_file.seek(0)
        scenarios = client_v2.generate_unit_test_scenarios(
            mental_model,
            rtl_file_contents=top_file.read(),
            unit_test_flow=unit_test_flow,
        )
        top_file.seek(0)
    except Exception as error:
        raise RuntimeError(f"Error generating unit test scenarios: {error}") from error

    try:
        top_file.seek(0)
        testbench = client_v2.generate_unit_tests(
            mental_model=mental_model,
            scenarios=scenarios,
            parsed_design=parsed_design,
            rtl_file_contents=top_file.read(),
            unit_test_flow=unit_test_flow,
        )
        top_file.seek(0)
    except Exception as error:
        raise RuntimeError(f"Error generating unit tests: {error}") from error

    return {
        "parsed_design": parsed_design,
        "mental_model": mental_model,
        "scenarios": scenarios,
        "testbench": testbench,
        "code": testbench["code"],
    }


class FormalSyntaxCorrection(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    client: ClientV2 = ClientV2()
    parsed_design: dict = {}
    mental_model: dict = {}
    syntax_check_type: str = "jasper"
    num_attempts: int = 3
    dump_output_logs: bool = False
    filter_compilable: bool = True
    formal_syntax_correction_logs: FormalSyntaxCorrectionLogs = (
        FormalSyntaxCorrectionLogs()
    )

    dependency_contents: dict = {}

    def syntax_check_codeblock(
        self,
        codeblock: str,
    ):
        try:
            tmp_dir = tempfile.mkdtemp()
            codeblock_name = re.findall(r"module\s+(\w+)", codeblock)[0]
            with open(os.path.join(tmp_dir, f"{codeblock_name}.sv"), "w") as file:
                file.write(codeblock)
            sva_file = open(os.path.join(tmp_dir, f"{codeblock_name}.sv"), "r")
            if self.syntax_check_type == "jasper":
                syntax_result = self.client.jasper_syntax_check(
                    module_name=self.parsed_design["name"],
                    clocks=[
                        clock["name"] for clock in self.parsed_design["clockPorts"]
                    ],
                    resets=[
                        reset["name"] for reset in self.parsed_design["resetPorts"]
                    ],
                    dut_files=[],
                    sva_files=[sva_file],
                )
            elif self.syntax_check_type == "slang":
                syntax_result = self.client.slang_compile_check(
                    files=[sva_file],
                )
            else:
                raise NotImplementedError(
                    f"Syntax check type {self.syntax_check_type} not implemented"
                )
        finally:
            # cleanup the temp folder
            sva_file.close()
            shutil.rmtree(tmp_dir)

        return SyntaxIssues(**syntax_result)

    def log_tag_codeblocks_with_syntax_issues(
        self,
        unique_idx: str,
        new_fixed_contexts: List[CorrectionContext],
        new_buggy_contexts: List[CorrectionContext],
        new_failed_contexts: List[CorrectionContext],
    ):
        correction_context_logs = (
            [
                FormalCorrectionContextLog(
                    code=correction_context.code,
                    syntax_issues=correction_context.syntax_issues,
                    type=SyntaxResultTypeEnum.syntax_passed.value,
                )
                for correction_context in new_fixed_contexts
            ]
            + [
                FormalCorrectionContextLog(
                    code=correction_context.code,
                    syntax_issues=correction_context.syntax_issues,
                    type=SyntaxResultTypeEnum.syntax_failed.value,
                )
                for correction_context in new_buggy_contexts
            ]
            + [
                FormalCorrectionContextLog(
                    code=correction_context.code,
                    syntax_issues=correction_context.syntax_issues,
                    type=SyntaxResultTypeEnum.failed_to_check.value,
                )
                for correction_context in new_failed_contexts
            ]
        )
        self.formal_syntax_correction_logs.syntax_check_history[unique_idx] = (
            correction_context_logs
        )

    def tag_codeblocks_with_syntax_issues(
        self, codeblocks: List[str], unique_idx: str
    ) -> Tuple[
        List[CorrectionContext], List[CorrectionContext], List[CorrectionContext]
    ]:
        fixed_correction_contexts = []
        buggy_correction_contexts = []
        failed_correction_contexts = []
        for codeblock in codeblocks:
            try:
                syntax_result: SyntaxIssues = self.syntax_check_codeblock(
                    codeblock,
                )
                correction_context = CorrectionContext(
                    code=codeblock, syntax_issues=syntax_result
                )
                if (len(syntax_result.errors) == 0) and (
                    len(syntax_result.warnings) == 0
                ):
                    fixed_correction_contexts.append(correction_context)
                else:
                    buggy_correction_contexts.append(correction_context)
            except Exception as e:
                correction_context = CorrectionContext(
                    code=codeblock,
                    syntax_issues=SyntaxIssues(
                        errors=[], warnings=[], full_logs=str(e)
                    ),
                )
                failed_correction_contexts.append(correction_context)

        self.log_tag_codeblocks_with_syntax_issues(
            unique_idx=unique_idx,
            new_fixed_contexts=fixed_correction_contexts,
            new_buggy_contexts=buggy_correction_contexts,
            new_failed_contexts=failed_correction_contexts,
        )
        return (
            fixed_correction_contexts,
            buggy_correction_contexts,
            failed_correction_contexts,
        )

    def log_correction_history(
        self,
        unique_idx: str,
        buggy_contexts: List[CorrectionContext],
        suggest_bug_fixes: List[str],
    ):
        assert len(buggy_contexts) == len(suggest_bug_fixes)
        correction_logs = []
        for idx, correction_context in enumerate(buggy_contexts):
            correction_logs.append(
                FormalCorrectionLog(
                    correction_context=correction_context,
                    suggested_correction=suggest_bug_fixes[idx],
                )
            )
        self.formal_syntax_correction_logs.correction_history[unique_idx] = (
            correction_logs
        )

    def fix_syntax_issues_loop(
        self, codeblocks: List[str], parsed_design: dict, mental_model: dict
    ):

        fixed_codeblocks = []
        potentially_buggy_codeblocks = codeblocks
        for syntax_check_attempt in range(self.num_attempts):
            unique_idx = str(syntax_check_attempt)
            if self.dump_output_logs:
                print(f"Syntax check attempt {syntax_check_attempt + 1}")
            new_fixed_contexts, new_buggy_contexts, new_failed_contexts = (
                self.tag_codeblocks_with_syntax_issues(
                    potentially_buggy_codeblocks, unique_idx=unique_idx
                )
            )
            fixed_codeblocks.extend([context.code for context in new_fixed_contexts])

            corrected_codes = self.client.fix_syntax_of_unit_tests(
                mental_model=mental_model,
                dut_parsed_model=parsed_design,
                correction_contexts=[
                    context.model_dump() for context in new_buggy_contexts
                ],
                unit_test_flow="Formal",
            )

            potentially_buggy_codeblocks = [
                corrected_code["code"] for corrected_code in corrected_codes
            ]
            potentially_buggy_codeblocks.extend(
                [context.code for context in new_failed_contexts]
            )

            self.log_correction_history(
                unique_idx=unique_idx,
                buggy_contexts=new_buggy_contexts,
                suggest_bug_fixes=[
                    corrected_code["code"] for corrected_code in corrected_codes
                ],
            )

        return fixed_codeblocks + potentially_buggy_codeblocks

    def fix_syntax_issues(
        self,
        top_rtl_path: str,
        dependency_paths: list[str],
        testbench_path: str,
    ) -> dict:
        """
        Corrects the syntax of the unit tests in the testbench.

        Args:
            top_rtl_path (str): The path to the RTL file.
            dependency_paths (list[str]): The paths to the dependency RTL files.
            testbench_path (str): The path to the testbench file.
            unit_test_flow (str, optional): The type of unit_test_flow. Defaults to "Formal".
        """

        client_v2 = ClientV2()
        # Load all the file contents
        top_file = open(top_rtl_path, "r")
        dependency_files: List[TextIO] = [open(file, "r") for file in dependency_paths]
        testbench_file = open(testbench_path, "r")
        testbench_contents = testbench_file.read()
        self.formal_syntax_correction_logs.code_to_check = testbench_contents
        try:
            parsed_design = client_v2.parse_design(
                top_file=top_file, dependency_files=dependency_files
            )
            self.formal_syntax_correction_logs.parsed_design = parsed_design
        except Exception as error:
            raise RuntimeError(f"Error parsing design: {error}")
        try:
            mental_model = client_v2.generate_mental_model(
                top_file=top_file, parsed_design=parsed_design
            )
            self.formal_syntax_correction_logs.mental_model = mental_model
        except Exception as error:
            raise RuntimeError(f"Error generating mental model: {error}")

        decomposed_unit_tests = client_v2.decompose_unit_tests(
            testbench_contents, unit_test_flow="Formal", dut_parsed_model=parsed_design
        )
        if "codeblocks" in decomposed_unit_tests:
            submodules = decomposed_unit_tests["codeblocks"]
        else:
            raise RuntimeError("Error: No codeblocks found in testbench")

        fixed_codeblocks = self.fix_syntax_issues_loop(
            submodules,
            parsed_design=parsed_design,
            mental_model=mental_model,
        )

        codeblocks_to_compose = fixed_codeblocks
        fixed_contexts, buggy_contexts, failed_contexts = (
            self.tag_codeblocks_with_syntax_issues(
                codeblocks_to_compose, unique_idx="LAST_CHECK"
            )
        )
        if self.filter_compilable:
            codeblocks_to_compose = [context.code for context in fixed_contexts]

        new_monitor = client_v2.compose_formal_unit_tests(
            codeblocks=codeblocks_to_compose,
            dut_parsed_model=parsed_design,
        )
        self.formal_syntax_correction_logs.code_to_show = new_monitor["code"]

        return self.formal_syntax_correction_logs.model_dump()


def generate_simulation_functional_coverage(
    top_rtl_path: str,
    dependency_paths: list[str],
    unit_test_flow: str = "Simulation",
):
    client_v2 = ClientV2()

    top_file = open(top_rtl_path, "r")
    dependency_files: List[TextIO] = [open(file, "r") for file in dependency_paths]
    try:
        parsed_design = client_v2.parse_design(
            top_file=top_file, dependency_files=dependency_files
        )
    except Exception as error:
        raise RuntimeError(f"Error parsing design: {error}") from error

    try:
        mental_model = client_v2.generate_mental_model(
            top_file=top_file,
            parsed_design=parsed_design,
            unit_test_flow=unit_test_flow,
        )
    except Exception as error:
        raise RuntimeError(f"Error generating mental model: {error}") from error

    try:
        scenarios = client_v2.generate_functional_coverage_scenarios(
            mental_model, unit_test_flow=unit_test_flow
        )
    except Exception as error:
        raise RuntimeError(
            f"Error generating functional coverage scenarios: {error}"
        ) from error

    try:
        functional_coverage = client_v2.generate_functional_coverage(
            mental_model=mental_model,
            scenarios=scenarios,
            parsed_design=parsed_design,
            unit_test_flow=unit_test_flow,
        )
    except Exception as error:
        raise RuntimeError(f"Error generating unit tests: {error}") from error

    if not functional_coverage or "code" not in functional_coverage:
        raise ValueError(
            "Expected return type of generate_functional_coverage is a dictionary that has key 'code'."
        )

    return functional_coverage


class CorrectionReqContext:
    def __init__(
        self,
        input_code: str,
        output_code: str,
        syntax_issues: SyntaxIssues = SyntaxIssues(),
        syntax_issues_checked: bool = False,
    ):
        self.input_code = input_code
        self.output_code = output_code
        self.syntax_issues = syntax_issues
        self.syntax_issues_checked = syntax_issues_checked

    def has_syntax_issue(self):
        return bool(self.syntax_issues)

    def __str__(self):
        return f"""Input: {
                self.input_code
            }\nOutput: {
                self.output_code
            }\nSyntax Issues: {
                self.syntax_issues
            }"""


class SyntaxCorrectionDetails:
    def __init__(
        self,
        orig_code: str,
        correction_history: list[CorrectionReqContext],
        final_code: str = "",
    ):
        self.orig_code = orig_code
        self.final_code = final_code
        self.correction_history: list[CorrectionReqContext] = correction_history

    def __str__(self):
        return f"""Original Code: {
                self.orig_code
            }\nFinal Code: {
                self.final_code
            }\nCorrection History: {
                self.correction_history
            }"""


class SyntaxCorrection:
    def __init__(self):
        self.codeblocks: List[SyntaxCorrectionDetails] = []

    def to_html(self):
        result = """<html>
            <head>
                <title>Syntax Correction</title>
                <style>
                td, th {
                    border: 1px solid black;
                }
                table {
                    width: 100%;
                    border-collapse: collapse;
                }
                .scrollable-content {
                    max-height: 300px;
                    max-width: 400px;
                    overflow: auto;
                }
                </style>
            </head>
            <body>
            """

        for idx, codeblock in enumerate(self.codeblocks):
            result += f"""<p style="margin-bottom: 0;"><strong>Syntax Correction for module/task/function: {idx+1}</strong></p>\n"""
            result += """<table border=1>"""
            result += """<colgroup><col style="width: 25%;"><col style="width: 25%;"><col style="width: 25%;"><col style="width: 25%;"></colgroup>"""
            result += """<tr><th>Attempt</th><th>Input</th><th>Output</th><th>Syntax Issues</th></tr>"""
            result += """<tr>"""
            result += """<td></td>"""
            result += f"""<td><div class="scrollable-content">{text_to_html(codeblock.orig_code)}</div></td>"""
            result += f"""<td><div class="scrollable-content">{text_to_html(codeblock.final_code)}</div></td>"""
            result += (
                f"""<td><div class="scrollable-content">{text_to_html("")}</div></td>"""
            )
            result += """</tr>"""
            for attempt, correction in enumerate(codeblock.correction_history):

                result += """<tr>"""
                result += f"""<td>{attempt}</td>"""
                result += f"""<td><div class="scrollable-content">{text_to_html(correction.input_code)}</div></td>"""
                result += f"""<td><div class="scrollable-content">{text_to_html(correction.output_code)}</div></td>"""
                result += f"""<td><div class="scrollable-content">{text_to_html(str(correction.syntax_issues))}</div></td>"""
                result += """</tr>"""
            result += """</table>"""

        result += """</body>\n</html>"""
        return result


def fix_syntax_issues_simulation(
    top_file: TextIO,
    dependency_files: List[TextIO],
    unit_tests_result: str,
    fun_cov_result: str,
    dump_output_logs: bool = False,
):
    client_v2 = ClientV2()
    try:
        top_file.seek(0)
        parsed_design = client_v2.parse_design(
            top_file=top_file, dependency_files=dependency_files
        )
    except Exception as error:
        raise RuntimeError(f"Error parsing design: {error}") from error

    try:
        top_file.seek(0)
        mental_model = client_v2.generate_mental_model(
            top_file=top_file,
            parsed_design=parsed_design,
            unit_test_flow="Simulation",
        )
    except Exception as error:
        raise RuntimeError(f"Error generating mental model: {error}") from error

    decomposed_unit_tests = client_v2.decompose_unit_tests(
        unit_tests_result + "\n" + fun_cov_result,
        unit_test_flow="Simulation",
        dut_parsed_model=parsed_design,
    )
    if "modules" not in decomposed_unit_tests:
        raise ValueError("Error: No modules found in testbench")
    if "tasks" not in decomposed_unit_tests:
        raise ValueError("Error: No tasks found in testbench")

    json_data = json.dumps(decomposed_unit_tests, indent=4)
    with open("output.json", "w") as file:
        file.write(json_data)

    syntax_correction: SyntaxCorrection = SyntaxCorrection()

    for cur_attempt in range(3):
        if cur_attempt == 0:
            # In the first attempt, we will check the syntax of all the modules
            for module_codeblock_raw in (
                decomposed_unit_tests["tasks"] + decomposed_unit_tests["modules"]
            ):
                syntax_correction.codeblocks.append(
                    SyntaxCorrectionDetails(
                        orig_code=module_codeblock_raw,
                        final_code=module_codeblock_raw,
                        correction_history=[
                            CorrectionReqContext(
                                input_code=module_codeblock_raw,
                                output_code=module_codeblock_raw,
                            )
                        ],
                    )
                )
        else:
            nr_of_modules_with_issues = 0
            # In subsequent attempts, we will only check the syntax of the modules that had issues in the previous attempt
            for module in syntax_correction.codeblocks:
                syntax_correction_details = module.correction_history[-1]
                if (
                    syntax_correction_details.syntax_issues_checked
                    and not syntax_correction_details.has_syntax_issue()
                ):
                    module.final_code = syntax_correction_details.output_code

                else:
                    nr_of_modules_with_issues += 1
                    last_attempt_code = syntax_correction_details.output_code
                    module.correction_history.append(
                        CorrectionReqContext(
                            input_code=last_attempt_code,
                            output_code=last_attempt_code,
                        )
                    )
            if nr_of_modules_with_issues == 0:
                # No modules had issues in the previous attempt, so we can break
                break

        for module in syntax_correction.codeblocks:
            if (
                # in the first attempt we dont want to skip syntax checking
                cur_attempt != 0
                # in the subsequent attempts, we want to skip if in the last attempt there was no syntax issues.
                and module.correction_history[-1].syntax_issues_checked
            ):
                continue

            module_codeblock_raw = module.correction_history[-1].input_code

            try:
                tmp_dir = tempfile.mkdtemp()
                codeblock_name = "test"
                matches = re.findall(r"module\s+(\w+)", module_codeblock_raw)
                if matches:
                    codeblock_name = matches[0]
                with open(os.path.join(tmp_dir, f"{codeblock_name}.sv"), "w") as file:
                    file.write(module_codeblock_raw)
                sva_file = open(os.path.join(tmp_dir, f"{codeblock_name}.sv"), "r")
                syntax_check_result = client_v2.simulation_syntax_check([sva_file])
            except Exception as error:
                raise RuntimeError(f"Error checking syntax: {error}") from error
            finally:
                # cleanup the temp folder
                sva_file.close()
                shutil.rmtree(tmp_dir)

            errors = []
            warnings = []
            if "log" not in syntax_check_result:
                raise ValueError(
                    "Unexpected syntax check result. Expected 'log' key in syntax check result."
                )
            else:
                if "errors" not in syntax_check_result["log"]:
                    raise ValueError(
                        "Unexpected syntax check result. Expected 'errors' key in syntax check result."
                    )
                else:
                    errors = syntax_check_result["log"]["errors"]

                if "warnings" not in syntax_check_result["log"]:
                    raise ValueError(
                        "Unexpected syntax check result. Expected 'warnings' key in syntax check result."
                    )
                else:
                    warnings = syntax_check_result["log"]["warnings"]

            module.correction_history[-1].syntax_issues_checked = True
            module.correction_history[-1].syntax_issues = SyntaxIssues(
                errors=errors, warnings=warnings
            )

            if len(errors) + len(warnings) != 0:
                correction_context = {
                    "code": module_codeblock_raw,
                    "syntax_issues": {"errors": errors, "warnings": warnings},
                    "other_issues": [],
                }

                corrected_codes = client_v2.fix_syntax_of_unit_tests(
                    mental_model=mental_model,
                    dut_parsed_model=parsed_design,
                    correction_contexts=[correction_context],
                    unit_test_flow="Simulation",
                )
                module.correction_history[-1].output_code = corrected_codes[0][
                    "output_code"
                ]
                module.final_code = corrected_codes[0]["output_code"]
            else:
                module.correction_history[-1].output_code = module_codeblock_raw
                module.final_code = module_codeblock_raw

        if dump_output_logs:
            print(f"Attempt {cur_attempt + 1} completed")

    tasks_to_compose = [
        cb.final_code
        for cb in syntax_correction.codeblocks[0 : len(decomposed_unit_tests["tasks"])]
    ]

    modules_to_compose = [
        cb.final_code
        for cb in syntax_correction.codeblocks[len(decomposed_unit_tests["tasks"]) :]
    ]

    unit_test_tb = client_v2.compose_simulation_unit_tests(
        tasks=tasks_to_compose,
        modules=modules_to_compose,
        dut_parsed_model=parsed_design,
    )

    if dump_output_logs:
        html_contents = syntax_correction.to_html()
        with open("output.html", "w") as file:
            file.write(html_contents)
        print("HTML output written to output.html")

        with open("unit_test_tb.sv", "w") as file:
            file.write(unit_test_tb)  # type: ignore

    return unit_test_tb


def run_formal(top_rtl_path: str, dependency_paths: list[str], sva_file_path: str):
    """
    Runs formal tests on the RTL file with the given SVA monitor.

    Args:
        top_rtl_path (str): The path to the RTL file.
        dependency_paths (list[str]): The paths to the dependency RTL files.
        sva_file_path (str): The path to the SVA monitor file.
    """
    client_v2 = ClientV2()

    top_file = open(top_rtl_path, "r")
    dependency_files: List[TextIO] = [open(file, "r") for file in dependency_paths]
    monitor_file = open(sva_file_path, "r")

    try:
        top_file.seek(0)
        for file in dependency_files:
            file.seek(0)
        parsed_design = client_v2.parse_design(
            top_file=top_file, dependency_files=dependency_files
        )

        top_file.seek(0)
        for file in dependency_files:
            file.seek(0)
        results = client_v2.run_formal_tests(
            module_name=parsed_design["name"] + "_monitor",
            clocks=[clock["name"] for clock in parsed_design["clockPorts"]],
            resets=[reset["name"] for reset in parsed_design["resetPorts"]],
            dut_files=[top_file] + dependency_files,
            sva_files=[monitor_file],
        )

        top_file.seek(0)
        for file in dependency_files:
            file.seek(0)
        return results
    except json.JSONDecodeError as error:
        raise RuntimeError(f"Error running formal tests: {error}") from error
    except Exception as error:
        raise RuntimeError(f"Unexpected error running formal tests: {error}") from error
